{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7e07bd-a14a-4486-be61-4be07778da16",
   "metadata": {},
   "source": [
    "# Site Crawler\n",
    "we are going to use this to build a site crawler that will crawl all pages in a site, and save each file in a markdown format\n",
    "\n",
    "if we can use langchain for this, and use it to grab things we will use it. ok so agentic web crawler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5b1b3-3665-4a6f-a077-875952b75479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3edf475-b0f5-414f-84d2-a6aeb28cbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_to_md.py\n",
    "# Minimal, robust website -> Markdown crawler\n",
    "# Usage:\n",
    "#   python crawl_to_md.py https://example.com --out out_md --max-pages 1000 --delay 0.5\n",
    "#   pip install -r requirements.txt\n",
    "\n",
    "import argparse, os, re, time, hashlib, queue, sys, datetime\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "import urllib.robotparser as robotparser\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c120b-0943-425c-ae01-d60faed44832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f9c77-0bd2-4a9d-af45-aafa63f1b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def norm_url(seed_netloc, url):\n",
    "    # drop fragments, normalize, enforce same host\n",
    "    url, _ = urldefrag(url.strip())\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        u = urlparse(url)\n",
    "        if not u.scheme:\n",
    "            return None\n",
    "        if u.netloc != seed_netloc:\n",
    "            return None\n",
    "        # strip default ports, normalize path\n",
    "        path = re.sub(r\"/+\", \"/\", u.path or \"/\")\n",
    "        # drop tracking query params (basic)\n",
    "        q = \"&\".join(sorted([p for p in (u.query or \"\").split(\"&\") if p and not p.lower().startswith((\"utm_\",\"gclid\",\"fbclid\"))]))\n",
    "        return f\"{u.scheme}://{u.netloc}{path}\" + (f\"?{q}\" if q else \"\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_probably_html(resp):\n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return \"text/html\" in ctype or \"application/xhtml\" in ctype or (not ctype and resp.text.startswith(\"<\"))\n",
    "\n",
    "def slugify_path(url):\n",
    "    u = urlparse(url)\n",
    "    # turn /a/b/ into a-b, fallback to hash if too short\n",
    "    path = u.path.strip(\"/\")\n",
    "    if not path:\n",
    "        path = \"index\"\n",
    "    path = re.sub(r\"[^a-zA-Z0-9\\-/_]\", \"-\", path)\n",
    "    path = re.sub(r\"/+\", \"/\", path)\n",
    "    slug = path.replace(\"/\", \"-\")\n",
    "    # include stable hash of query to avoid collisions\n",
    "    h = hashlib.sha1((u.path + \"?\" + (u.query or \"\")).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{slug}-{h}.md\"\n",
    "\n",
    "def front_matter(title, url):\n",
    "    ts = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
    "    return f\"---\\ntitle: \\\"{title}\\\"\\nsource: \\\"{url}\\\"\\ncrawled_at: \\\"{ts}\\\"\\n---\\n\\n\"\n",
    "\n",
    "def extract_title(soup):\n",
    "    if soup.title and soup.title.string:\n",
    "        return soup.title.string.strip()\n",
    "    h1 = soup.find(\"h1\")\n",
    "    return (h1.get_text(strip=True) if h1 else \"Untitled\")\n",
    "\n",
    "def discover_links(base_url, soup, seed_netloc):\n",
    "    links = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        absu = urljoin(base_url, href)\n",
    "        n = norm_url(seed_netloc, absu)\n",
    "        if n:\n",
    "            links.add(n)\n",
    "    return links\n",
    "\n",
    "def parse_sitemaps(seed, session):\n",
    "    # robots -> Sitemap: URLs -> parse XML -> loc entries\n",
    "    urls = set()\n",
    "    try:\n",
    "        rp = session.get(urljoin(seed, \"/robots.txt\"), timeout=10)\n",
    "        if rp.status_code == 200:\n",
    "            for line in rp.text.splitlines():\n",
    "                if line.lower().startswith(\"sitemap:\"):\n",
    "                    sm = line.split(\":\",1)[1].strip()\n",
    "                    urls |= parse_sitemap_xml(sm, session)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return urls\n",
    "\n",
    "def parse_sitemap_xml(url, session):\n",
    "    out = set()\n",
    "    try:\n",
    "        r = session.get(url, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return out\n",
    "        tree = ET.fromstring(r.content)\n",
    "        ns = {\"sm\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
    "        # urlset\n",
    "        for loc in tree.findall(\".//sm:url/sm:loc\", ns):\n",
    "            if loc.text:\n",
    "                out.add(loc.text.strip())\n",
    "        # nested sitemaps\n",
    "        for sm in tree.findall(\".//sm:sitemap/sm:loc\", ns):\n",
    "            if sm.text:\n",
    "                out |= parse_sitemap_xml(sm.text.strip(), session)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "def allowed_by_robots(seed, ua):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(urljoin(seed, \"/robots.txt\"))\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return rp\n",
    "\n",
    "def backoff_sleep(attempt, base_delay):\n",
    "    # exponential backoff with cap\n",
    "    time.sleep(min(base_delay * (2 ** attempt), 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baabd38-1c88-48cf-8689-b801a103788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main crawl ----------\n",
    "def crawl(seed, out_dir, max_pages, delay, timeout, user_agent, include_sitemap):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    seed = seed.rstrip(\"/\")\n",
    "    seed_parsed = urlparse(seed)\n",
    "    seed_netloc = seed_parsed.netloc\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": user_agent})\n",
    "    robots = allowed_by_robots(seed, user_agent)\n",
    "\n",
    "    q = queue.Queue()\n",
    "    visited = set()\n",
    "    enq = set()\n",
    "\n",
    "    # seed queue\n",
    "    q.put(seed); enq.add(seed)\n",
    "\n",
    "    # optional: pre-seed with sitemap URLs\n",
    "    if include_sitemap:\n",
    "        for u in parse_sitemaps(seed, session):\n",
    "            n = norm_url(seed_netloc, u)\n",
    "            if n and n not in enq:\n",
    "                q.put(n); enq.add(n)\n",
    "\n",
    "    saved = 0\n",
    "    attempts = {}\n",
    "\n",
    "    while not q.empty() and saved < max_pages:\n",
    "        url = q.get()\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        if robots and hasattr(robots, \"can_fetch\") and not robots.can_fetch(user_agent, url):\n",
    "            continue\n",
    "\n",
    "        # polite delay\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # fetch with retry\n",
    "        for attempt in range(4):\n",
    "            try:\n",
    "                resp = session.get(url, timeout=timeout, allow_redirects=True)\n",
    "                status = resp.status_code\n",
    "                if status in (429, 500, 502, 503, 504):\n",
    "                    backoff_sleep(attempt, 1.0); continue\n",
    "                if status != 200:\n",
    "                    break\n",
    "                if not is_probably_html(resp):\n",
    "                    break\n",
    "                html = resp.text\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                # extract & enqueue links\n",
    "                for link in discover_links(resp.url, soup, seed_netloc):\n",
    "                    if link not in enq:\n",
    "                        q.put(link); enq.add(link)\n",
    "\n",
    "                # convert to markdown\n",
    "                title = extract_title(soup)\n",
    "                # remove script/style/nav/footer noise (basic)\n",
    "                for tag in soup([\"script\",\"style\",\"noscript\"]):\n",
    "                    tag.decompose()\n",
    "                # Optional: drop navs/footers by role/semantic (lightweight heuristic)\n",
    "                for tag in soup.find_all(attrs={\"role\":\"navigation\"}): tag.decompose()\n",
    "                for tag in soup.find_all([\"nav\",\"footer\"]): tag.decompose()\n",
    "\n",
    "                body_html = str(soup.body or soup)\n",
    "                markdown = md(body_html, heading_style=\"ATX\", strip=[\"img\"])  # skip images by default\n",
    "\n",
    "                # write file\n",
    "                fname = slugify_path(resp.url)\n",
    "                path = os.path.join(out_dir, fname)\n",
    "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(front_matter(title, resp.url))\n",
    "                    # keep the H1 if present as title inside too (optional)\n",
    "                    f.write(markdown.strip() + \"\\n\")\n",
    "                saved += 1\n",
    "                break\n",
    "            except requests.RequestException:\n",
    "                backoff_sleep(attempt, 1.0)\n",
    "                continue\n",
    "            except Exception:\n",
    "                # skip malformed pages\n",
    "                break\n",
    "\n",
    "    print(f\"Done. Saved {saved} markdown files to {out_dir}. Visited {len(visited)} URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f91b36-f0f7-4e7f-a85d-7ba1f47f28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     ap = argparse.ArgumentParser(description=\"Crawl a website and save pages as Markdown.\")\n",
    "#     ap.add_argument(\"seed\", help=\"Seed URL (e.g., https://example.com)\")\n",
    "#     ap.add_argument(\"--out\", dest=\"out_dir\", default=\"site_md\", help=\"Output directory\")\n",
    "#     ap.add_argument(\"--max-pages\", type=int, default=1000, help=\"Maximum pages to save\")\n",
    "#     ap.add_argument(\"--delay\", type=float, default=0.5, help=\"Politeness delay between requests (seconds)\")\n",
    "#     ap.add_argument(\"--timeout\", type=int, default=15, help=\"HTTP request timeout (seconds)\")\n",
    "#     ap.add_argument(\"--ua\", dest=\"user_agent\", default=\"SiteToMarkdownBot/1.0 (+https://example.com)\", help=\"User-Agent string\")\n",
    "#     ap.add_argument(\"--no-sitemap\", action=\"store_true\", help=\"Do not pre-seed URLs from sitemap\")\n",
    "#     args = ap.parse_args()\n",
    "\n",
    "#     crawl(\n",
    "#         seed=args.seed,\n",
    "#         out_dir=args.out_dir,\n",
    "#         max_pages=args.max_pages,\n",
    "#         delay=args.delay,\n",
    "#         timeout=args.timeout,\n",
    "#         user_agent=args.user_agent,\n",
    "#         include_sitemap=(not args.no_sitemap),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94623c9-e6fb-4ea1-8615-c8946d230cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = \"https://help.itglue.kaseya.com/help/Content/0-HOME/Home.htm\"\n",
    "out_dir = \"out_md\"\n",
    "\n",
    "crawl(\n",
    "    seed=seed_url,\n",
    "    out_dir=out_dir,\n",
    "    max_pages=100,\n",
    "    delay=0.5,\n",
    "    timeout=15,\n",
    "    user_agent=\"MyCrawlerBot/1.0\",\n",
    "    include_sitemap=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
