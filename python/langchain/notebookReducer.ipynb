{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcabf777-e60c-4d6a-abc8-eb5de4ae62f5",
   "metadata": {},
   "source": [
    "# Notebook Reducer\n",
    "This notebook allows us to grab text files in one folder, \n",
    "annotate them, and move them to another folder in markdown.\n",
    "\n",
    "we should not delete the notes but, we should definitely keep a index of the notes already done so we can go back and do them if something fails\n",
    "\n",
    "the folders will be inbox(files that were put in), knowledge(the files coming out), config(configuration files needed)\n",
    "\n",
    "the system will categorize the notes. and if it finds a note with the same category, it add the note to it.\n",
    "to save money and space probably we will do all the reduction of notes first, then we will write something else to refine the output folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af0ea5-f142-4ec0-bdc4-61bf07f7115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "organize_notes_chatgpt.py\n",
    "Summarize, categorize, and organize .txt files into Markdown folders using ChatGPT (via LangChain).\n",
    "ChatGPT generates the ENTIRE Markdown (with YAML front-matter) for both new notes and merges.\n",
    "\n",
    "Setup:\n",
    "  pip install langchain langchain-openai pydantic PyYAML\n",
    "  export OPENAI_API_KEY=...\n",
    "Run:\n",
    "  python organize_notes_chatgpt.py --src ./inbox --dest ./knowledge --dry-run\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19da9fe9-42b6-4c75-8948-30a47aa44143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import asyncio\n",
    "import datetime as dt\n",
    "import glob\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ebc4d4-1f08-455a-9e84-3668bb0154e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57797870-2df0-43c9-b116-3655d89aa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return dt.datetime.now().astimezone().replace(microsecond=0).isoformat()\n",
    "\n",
    "def sha256(s: str) -> str:\n",
    "    import hashlib as _h\n",
    "    return _h.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def write_text(path: Path, text: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_markdown_files(root: Path) -> List[Path]:\n",
    "    return [Path(p) for p in glob.glob(str(root / \"**\" / \"*.md\"), recursive=True)]\n",
    "\n",
    "FM_BOUNDARY = re.compile(r\"^---\\s*$\", re.M)\n",
    "\n",
    "def parse_front_matter(md_text: str) -> Tuple[dict, str]:\n",
    "    parts = FM_BOUNDARY.split(md_text)\n",
    "    if len(parts) >= 3:\n",
    "        yml = parts[1]\n",
    "        body = \"\".join(parts[2:]).lstrip(\"\\n\")\n",
    "        try:\n",
    "            fm = yaml.safe_load(yml) or {}\n",
    "        except Exception:\n",
    "            fm = {}\n",
    "        return fm, body\n",
    "    return {}, md_text\n",
    "\n",
    "def path_for(dest: Path, folder: str, slug: str) -> Path:\n",
    "    # Normalize folder components to safe path\n",
    "    folder = \"/\".join([re.sub(r\"[^A-Za-z0-9._ -]\", \"\", c).strip().strip(\".\") for c in folder.split(\"/\") if c.strip()])\n",
    "    slug = re.sub(r\"[^a-z0-9-]\", \"-\", slug.lower()).strip(\"-\")\n",
    "    return dest / folder / f\"{slug}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95eb74d8-87ae-4450-b2e9-3ac51141af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LLM Chains (ChatGPT writes markdown)\n",
    "# ---------------------------\n",
    "\n",
    "SYSTEM_NEW = \"\"\"You are a meticulous knowledge base editor.\n",
    "Create a COMPLETE Markdown document with YAML front-matter for a personal knowledge base.\n",
    "Strict requirements:\n",
    "- Begin with YAML front-matter delimited by '---' on its own lines.\n",
    "- Front-matter MUST include:\n",
    "  title (string),\n",
    "  slug (kebab-case, <=64 chars, filesystem-safe),\n",
    "  folder (e.g., \"Domain/Subdomain\" with 1–2 levels, no leading slash),\n",
    "  tags (string list),\n",
    "  merge_key (stable canonical key for this topic),\n",
    "  created_at (ISO 8601),\n",
    "  updated_at (ISO 8601).\n",
    "- Body structure:\n",
    "  '# {title}'\n",
    "  '## Summary' (3–6 sentences)\n",
    "  '## Key Points' (5–10 bullets, terse, no duplicates)\n",
    "  Optionally '## Source Text' in fenced block if requested.\n",
    "- Avoid PII in title/tags. Prefer specific, concrete titles. If no clear category, folder='Misc'.\n",
    "- Return ONLY the Markdown file content.\n",
    "\"\"\"\n",
    "\n",
    "USER_NEW = \"\"\"Create the complete Markdown from this raw text.\n",
    "\n",
    "Raw text:\n",
    "---\n",
    "{content}\n",
    "---\n",
    "\n",
    "Include full source text section: {include_fulltext}\n",
    "Timestamps to set (ISO): created_at={created_at}, updated_at={updated_at}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_MERGE = \"\"\"You are a careful editor merging a new source into an existing Markdown note.\n",
    "Return a FULLY UPDATED Markdown file that:\n",
    "- Preserves and updates YAML front-matter. Keep existing title/slug/folder/merge_key unless the new source clearly improves them; always keep merge_key the same.\n",
    "- Update 'updated_at' to the provided ISO timestamp.\n",
    "- Deduplicate bullets and facts across versions; keep content crisp.\n",
    "- Keep existing sections; append an '## Update — {timestamp}' section summarizing new information in 2–5 bullets.\n",
    "- If '## Key Points' exists, refresh the list to include the most important, deduped points (max 12 bullets).\n",
    "- If requested, include a 'New Source Text' fenced block under the update.\n",
    "- Maintain clean Markdown. Return ONLY the Markdown file.\n",
    "\"\"\"\n",
    "\n",
    "USER_MERGE = \"\"\"Here is the current Markdown file:\n",
    "\n",
    "<current_md>\n",
    "{current_md}\n",
    "</current_md>\n",
    "\n",
    "Here is the new raw text to merge:\n",
    "<new_text>\n",
    "{new_text}\n",
    "</new_text>\n",
    "\n",
    "Include 'New Source Text' block: {include_fulltext}\n",
    "New updated_at to set (ISO): {updated_at_iso}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff6be38-29b5-4386-8d69-c7448f311619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm(model: str, api_key: Optional[str]) -> ChatOpenAI:\n",
    "    # temperature low for determinism; you can flip to 0.3 if you want more creative tags/folders\n",
    "    return ChatOpenAI(model=model, temperature=0.2, api_key=api_key)\n",
    "\n",
    "async def generate_markdown(llm: ChatOpenAI, content: str, include_fulltext: bool) -> str:\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", SYSTEM_NEW),\n",
    "         (\"user\", USER_NEW)]\n",
    "    )\n",
    "    created = now_iso()\n",
    "    updated = created\n",
    "    chain = prompt | llm\n",
    "    res = await chain.ainvoke({\n",
    "        \"content\": content,\n",
    "        \"include_fulltext\": str(bool(include_fulltext)).lower(),\n",
    "        \"created_at\": created,\n",
    "        \"updated_at\": updated,\n",
    "    })\n",
    "    return res.content.strip()\n",
    "\n",
    "async def merge_markdown(llm: ChatOpenAI, current_md: str, new_text: str, include_fulltext: bool) -> str:\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", SYSTEM_MERGE),\n",
    "         (\"user\", USER_MERGE)]\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    res = await chain.ainvoke({\n",
    "        \"current_md\": current_md,\n",
    "        \"new_text\": new_text,\n",
    "        \"include_fulltext\": str(bool(include_fulltext)).lower(),\n",
    "        \"updated_at_iso\": now_iso(),\n",
    "    })\n",
    "    return res.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e98e04-5a34-4939-8386-fb58ce8c4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Pipeline\n",
    "# ---------------------------\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, src: Path, dest: Path, model: str, api_key: Optional[str],\n",
    "                 dry_run: bool, include_fulltext: bool, dup_action: str):\n",
    "        self.src = src\n",
    "        self.dest = dest\n",
    "        self.llm = make_llm(model, api_key)\n",
    "        self.dry_run = dry_run\n",
    "        self.include_fulltext = include_fulltext\n",
    "        self.dup_action = dup_action  # 'archive' | 'delete' | 'none'\n",
    "        ensure_dir(dest)\n",
    "\n",
    "    def _find_by_merge_key(self, merge_key: str) -> List[Path]:\n",
    "        hits = []\n",
    "        for md in list_markdown_files(self.dest):\n",
    "            fm, _ = parse_front_matter(read_text(md))\n",
    "            if fm.get(\"merge_key\") == merge_key:\n",
    "                hits.append(md)\n",
    "        return hits\n",
    "\n",
    "    def _extract_route(self, md_text: str) -> Tuple[str, str, str]:\n",
    "        fm, _ = parse_front_matter(md_text)\n",
    "        folder = fm.get(\"folder\") or \"Misc\"\n",
    "        slug = fm.get(\"slug\") or \"untitled\"\n",
    "        merge_key = fm.get(\"merge_key\") or slug\n",
    "        return folder, slug, merge_key\n",
    "\n",
    "    async def _process_new_or_merge(self, txt_path: Path):\n",
    "        raw = read_text(txt_path).strip()\n",
    "        if not raw:\n",
    "            print(f\"[skip empty] {txt_path}\")\n",
    "            return\n",
    "        chk = sha256(raw)\n",
    "\n",
    "        # First, create a candidate MD so we can read merge_key/folder/slug\n",
    "        md_candidate = await generate_markdown(self.llm, raw, self.include_fulltext)\n",
    "        folder, slug, merge_key = self._extract_route(md_candidate)\n",
    "\n",
    "        existing = self._find_by_merge_key(merge_key)\n",
    "\n",
    "        if not existing:\n",
    "            out_path = path_for(self.dest, folder, slug)\n",
    "            if self.dry_run:\n",
    "                print(f\"[create] {out_path} (dry-run)\")\n",
    "            else:\n",
    "                # inject checksum/source_path into front-matter\n",
    "                fm, body = parse_front_matter(md_candidate)\n",
    "                fm[\"checksum\"] = chk\n",
    "                fm[\"source_path\"] = str(txt_path.resolve())\n",
    "                # reserialize\n",
    "                yml = yaml.safe_dump(fm, sort_keys=False, allow_unicode=True).strip()\n",
    "                final_md = f\"---\\n{yml}\\n---\\n\\n{body}\"\n",
    "                write_text(out_path, final_md)\n",
    "                print(f\"[created] {out_path}\")\n",
    "        else:\n",
    "            # Choose primary: prefer one whose folder/slug match candidate route\n",
    "            target_path = path_for(self.dest, folder, slug)\n",
    "            primary = None\n",
    "            for mdp in existing:\n",
    "                if mdp.resolve() == target_path.resolve():\n",
    "                    primary = mdp\n",
    "                    break\n",
    "            if primary is None:\n",
    "                primary = existing[0]\n",
    "\n",
    "            current_md = read_text(primary)\n",
    "            merged_md = await merge_markdown(self.llm, current_md, raw, self.include_fulltext)\n",
    "\n",
    "            if self.dry_run:\n",
    "                print(f\"[update] {primary} (dry-run)\")\n",
    "            else:\n",
    "                # ensure we persist checksum of latest source and update updated_at (already set by model)\n",
    "                fm, body = parse_front_matter(merged_md)\n",
    "                fm = fm or {}\n",
    "                fm[\"checksum_last_source\"] = chk\n",
    "                fm.setdefault(\"source_paths\", [])\n",
    "                # merge source_paths as a set\n",
    "                try:\n",
    "                    paths = set(fm.get(\"source_paths\", []))\n",
    "                except Exception:\n",
    "                    paths = set()\n",
    "                paths.add(str(txt_path.resolve()))\n",
    "                fm[\"source_paths\"] = sorted(paths)\n",
    "                yml = yaml.safe_dump(fm, sort_keys=False, allow_unicode=True).strip()\n",
    "                final_md = f\"---\\n{yml}\\n---\\n\\n{body}\"\n",
    "                write_text(primary, final_md)\n",
    "                print(f\"[updated] {primary}\")\n",
    "\n",
    "            # Handle duplicates (other files with same merge_key)\n",
    "            for mdp in existing:\n",
    "                if mdp.resolve() == primary.resolve():\n",
    "                    continue\n",
    "                if self.dup_action == \"none\":\n",
    "                    continue\n",
    "                if self.dry_run:\n",
    "                    print(f\"[dup-{self.dup_action}] {mdp} -> {primary} (dry-run)\")\n",
    "                    continue\n",
    "                if self.dup_action == \"delete\":\n",
    "                    mdp.unlink(missing_ok=True)\n",
    "                    print(f\"[deleted-dup] {mdp}\")\n",
    "                else:\n",
    "                    # archive\n",
    "                    archive_dir = primary.parent / \"_archive\"\n",
    "                    ensure_dir(archive_dir)\n",
    "                    dst = archive_dir / mdp.name\n",
    "                    shutil.move(str(mdp), str(dst))\n",
    "                    print(f\"[archived-dup] {mdp} -> {dst}\")\n",
    "\n",
    "        # Move processed source\n",
    "        processed_dir = self.dest / \"_processed_src\"\n",
    "        ensure_dir(processed_dir)\n",
    "        if self.dry_run:\n",
    "            print(f\"[move-src] {txt_path} -> {processed_dir / txt_path.name} (dry-run)\")\n",
    "        else:\n",
    "            shutil.move(str(txt_path), str(processed_dir / txt_path.name))\n",
    "\n",
    "    async def run(self):\n",
    "        txts = sorted([p for p in self.src.glob(\"**/*.txt\") if p.is_file()])\n",
    "        if not txts:\n",
    "            print(f\"[no-input] No .txt files under {self.src}\")\n",
    "            return\n",
    "        sem = asyncio.Semaphore(4)\n",
    "\n",
    "        async def task(p: Path):\n",
    "            async with sem:\n",
    "                await self._process_new_or_merge(p)\n",
    "\n",
    "        await asyncio.gather(*[task(p) for p in txts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbeb221-5957-46ad-8524-69cacc2ba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        src=src,\n",
    "        dest=dest,\n",
    "        model=model,\n",
    "        api_key=openai_api_key,\n",
    "        dry_run=dry_run,\n",
    "        include_fulltext=include_fulltext,\n",
    "        dup_action=dup_action,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
